#ols사용 : 가장 기본적인 결정론적 선형회귀방법. 최적의 예측값을 찾기 위한 함수를 찾음 w와 b값. - 불확실성이 있다.(오차가 있을 수 있다. 잔차, 오차 등)
#데이터 분석은 연역적, 귀납적 추론 중에 귀납적 추론을 따른다 

import pandas as pd
import statsmodels.formula.api as smf
#numpy, pandas, matplotlib 중요
#사이파이, 사키이런, statsmodels 모델 중요

df = pd.read_csv("../testdata/drinking_water.csv")
print(df.head(3))
print(df.corr())
#          친밀도       적절성       만족도
#친밀도  1.000000  0.499209  0.467145
#적절성  0.499209  1.000000  0.766853
#만족도  0.467145  0.766853  1.000000

#상관관계가 0.05보다 높으므로 해당 모데이터는 유의하다

#회귀분석 : 상관 관계가 가장 높은 만족도와 적절성으로 "인과관계가 있다" 라는 가정하에 시도

model = smf.ols(formula = '만족도 ~ 적절성', data = df).fit() 
#fit()은 추세선(회귀선)이 w와 b값 예측값과 실제값 차이, 잔차가 최소가 되는 값을 찾는다.
#결과 안 좋으면 w와 b값 바꿔가면서 계속 시도하는 것. 학습 데이터는 입력 데이터로 계속 학습하는 것 이지만 너무 빨라서 안 보이는 것
#해당 데이터로도 귀무와 대립 가설을 작성할 수도 있다
#적절성은 만족도에 영향을 준다 - 대립

print(model.summary())
#OLS Regression Results
#종속변수는 만족도
#No. Observations:  264  관찰계수는 264개
#R-squared:  0.588 설명력 독립변수가 종속변수의 분산을 0.588만큼 설명한다    설명된분산/종속변수의 전체 분산
#결정계수 R은 1에 가까울 수록 설명력이 높다
#모델에 대해서 100% 완벽하게 설명할 수는 없다 - 참고용
#상관계수r를 제곱하면 결정계수R이 나옴 혹은 1-SSR/SST로 하면 나옴  SSE/SST도 가능

#기울기와 절편인 w와 b가 같아서 같은 회귀선이 나오더라도 결정계수 R은 다르게 나올 수가 있다 (데이터의 산포도에 따라 다르게 나옴)

#변수가 많을 수록 데이터의 정확도는 올라간다
#독립변수가 많을 수록 수정된 결정계수(Adj. R-squared)를 사용 하나일 떄는 그냥 결정계수 사용

#F-statistic:  374.0 는 t에 의해 나오게 되고 t는 coef/std err 로 나온다
#Prob (F-statistic): 2.24e-52 Pvalue로 모델의 유의함을 설명해줌. 0.05보다 작아야함.

#모델의 성능, 독립 변수를 몇 가지를 선택해야 하는지를 알기 위해 AIC와 BIC 사용
#AIC: 418.9  0에 가까울 수록 좋다 - 독립 변수 여러 가지를 사용해 보고 가장 작은 것을 사용하면 된다, python에서는 직접 해봐야 한다. R에서는 전진 선택법, 후진 소거법 사용하면 됨
#BIC: 426.0  

#[0.025      0.975] 95%의 신뢰구간 안에 절편과 기울기가 나온다
#y = coef(적절성) * x + coef(Intercept)
#y값은 x값의 기울기 만큼 증가한다.


#모집단에서 표본을 뽑음. 평균에서 약간씩의 차이는 있음
#이때 표본 평균들의 차이를 표준 오차라고 한다(std err)
#표준 오차가 작으면 참(모집단과 표본에 차이가 적다. 모집단의 모수와 표본 통계량 값에 차이가 적다) 값에 가깝다. -> 산포도가 집중되어 있음. 분산의 설명력이 낮다 pvalue > 0.05
#표준 오차가 크면 참(모집단과 표본에 차이가 크다.) 값에 멀다. -> 산포도가 분산되어 있음. 분산의 설명력이 높다 pvalue < 0.05

#모집단간의 평균 차이가 t-test이다(집단 1과 집단 2의 차이 = t) t값과 p값은 반비례한다
#t값은  기울기/표준오차 로 구할 수 있고 다음과 같다 (집단1평균-집단2평균)/표준오차
#표준오차가 커지면 t값이 작아지게 된다 표준오차가 커지면 p값은 커진다 -> 모델의 유의함이 떨어진다
#표준오차가 작아지면 t값이 커지게 되고 p값은 작이진다 -> 모델이 유의하다

#귀무가설은 기울기가 0이라는 가정하에 설정하고 대립가설은 기울기가 0이 아니라는 것을 설정한다
#즉, 어떠한 독립변수 x에 대해 종속변수 y가 변하는 관계가 있다는 것을 증명하기 위하여 
#기울기 != 0 일때 유의하다 회귀선은 x가 변화할 때 y도 변화한다


#Skew: -0.328 왜도값이 0이 나오면 정규분포를 따른다. 음수 값이면 왼쪽으로 꼬리가 길고 양수이면 오른 쪽으로 꼬리가 길다 = 편향되어 있다
#Kurtosis: 4.012 첨도 정규분포일 때는 0의 값을 가진다. 종의 모양을 띄고 있다. 음수일 때는 종의 모양이 낮고 양수일 때는 종의 모양이 뾰족하다
#첨도의 종의 모양이 뾰족할 수록 데이터는 모여있다

#Durbin-Watson: 2.185 잔차의 독립성을 설명할 때 사용 2에 가까울 수록 좋다 0~4의 값이 나옴. 2에 가까울 수록 독립적

#Cond. No. 13.4 다중공선성 독립변수가 하나일 때는 의미가 없다. 값이 클 수록 다중곤선성을 의심
#여러 독립변수가 있을 때 독립변수 끼리의 겹치는 부분

#R-squared: 0.588  보통 15~20% 정도 나온다. 결정계수가 작더라도 인과관계가 보이면 해볼수 있다 

print('결정계수 R : ',model.rsquared) #0.5880630629464404
print('p-value : ', model.pvalues) #2.235345e-52
print('예측값 : ', model.predict()[:5]) #[3.73596305 2.99668687 3.73596305 2.25741069 2.25741069]
print('실제값 : ', df.만족도[:5].values) #[3 2 4 2 2]



#시각화
import numpy as np
import matplotlib.pyplot as plt

plt.scatter(df.적절성, df.만족도)
slope, intercept = np.polyfit(df.적절성, df.만족도, 1)
plt.plot(df.적절성, df.적절성 * slope + intercept, 'b') #y = coef(적절성) * x + coef(Intercept)
plt.show()


